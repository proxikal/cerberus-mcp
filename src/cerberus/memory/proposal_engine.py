"""
Phase 3: Session Proposal

Generates memory proposals from correction clusters using template-based rules.
LLM refinement is OPTIONAL - system works 100% without Ollama.

Zero token cost for template-based generation, 0-500 tokens if LLM enabled.
"""

from dataclasses import dataclass, field
from datetime import datetime
from typing import List, Optional
import uuid
import re


@dataclass
class MemoryProposal:
    """Represents a proposed memory to be stored."""
    id: str  # "prop-abc123de"
    category: str  # "preference", "rule", "correction"
    scope: str  # "universal", "language:go", "project:cerberus"
    content: str  # The actual memory text
    rationale: str  # Why this should be stored
    source_variants: List[str] = field(default_factory=list)  # Original corrections
    confidence: float = 0.0
    priority: int = 3  # 1=critical, 2=high, 3=medium

    def to_dict(self) -> dict:
        """Convert to dictionary for JSON serialization."""
        return {
            "id": self.id,
            "category": self.category,
            "scope": self.scope,
            "content": self.content,
            "rationale": self.rationale,
            "source_variants": self.source_variants,
            "confidence": self.confidence,
            "priority": self.priority
        }


@dataclass
class AgentProposal:
    """Represents a proposal generated by agent self-learning (Phase 10)."""
    id: str
    category: str
    scope: str
    content: str
    rationale: str
    confidence: float = 0.0
    priority: int = 3

    def to_dict(self) -> dict:
        """Convert to dictionary for JSON serialization."""
        return {
            "id": self.id,
            "category": self.category,
            "scope": self.scope,
            "content": self.content,
            "rationale": self.rationale,
            "confidence": self.confidence,
            "priority": self.priority
        }


@dataclass
class SessionSummary:
    """Summary of session proposals."""
    session_id: str
    timestamp: datetime
    project: Optional[str] = None
    proposals: List[MemoryProposal] = field(default_factory=list)
    total_corrections: int = 0
    total_proposed: int = 0

    def to_dict(self) -> dict:
        """Convert to dictionary for JSON serialization."""
        return {
            "session_id": self.session_id,
            "timestamp": self.timestamp.isoformat(),
            "project": self.project,
            "proposals": [p.to_dict() for p in self.proposals],
            "total_corrections": self.total_corrections,
            "total_proposed": self.total_proposed
        }


class ProposalEngine:
    """
    Generate memory proposals from correction clusters.

    PRIMARY: Template-based rules (no dependencies, instant)
    OPTIONAL: LLM enhancement (if Ollama available AND enabled)
    """

    # Language detection keywords
    LANG_KEYWORDS = {
        "go": ["panic", "goroutine", "defer", "chan", "go func", "go mod", "golang"],
        "python": ["except", "async def", "import", "def ", "__init__", "pytest", "pip"],
        "typescript": ["interface", "type ", "promise", "tsx", "namespace", "enum"],
        "javascript": ["const ", "let ", "function", "=>", "async", "npm"],
        "rust": ["unsafe", "unwrap", "borrow", "impl", "fn ", "mut", "cargo"]
    }

    # Universal keywords (applies everywhere)
    UNIVERSAL_KEYWORDS = [
        "token", "file size", "line limit", "concise", "short",
        "verbose", "summary", "terse", "split", "200 lines",
        "ai", "agent", "llm", "context", "output"
    ]

    # Project-specific indicators
    PROJECT_INDICATORS = [
        "this project", "here", "our", "we use", "portal",
        "component", "module", "service", "endpoint"
    ]

    def __init__(self, use_llm: bool = False, max_proposals: int = 5):
        """
        Args:
            use_llm: If True AND Ollama is available, use LLM for refinement.
                     Default is False - template-based works well.
            max_proposals: Maximum number of proposals to generate (default: 5)
        """
        self.use_llm = use_llm
        self.max_proposals = max_proposals

    def generate_proposals(
        self,
        clusters: List,
        project: Optional[str] = None
    ) -> SessionSummary:
        """
        Generate memory proposals from correction clusters.

        Args:
            clusters: List of CorrectionCluster from Phase 2
            project: Optional project name for scope inference

        Returns:
            SessionSummary with proposals
        """
        if not clusters:
            return SessionSummary(
                session_id=self._gen_session_id(),
                timestamp=datetime.now(),
                project=project,
                proposals=[],
                total_corrections=0,
                total_proposed=0
            )

        # Sort clusters by priority (frequency * confidence)
        sorted_clusters = sorted(
            clusters,
            key=lambda c: c.frequency * c.confidence,
            reverse=True
        )

        # Take top N clusters
        top_clusters = sorted_clusters[:self.max_proposals]

        # Generate proposals using templates (PRIMARY)
        proposals = []
        for i, cluster in enumerate(top_clusters):
            proposal = self._create_proposal(cluster, project, priority=i+1)
            if proposal:
                proposals.append(proposal)

        return SessionSummary(
            session_id=self._gen_session_id(),
            timestamp=datetime.now(),
            project=project,
            proposals=proposals,
            total_corrections=sum(c.frequency for c in clusters),
            total_proposed=len(proposals)
        )

    def _create_proposal(
        self,
        cluster,
        project: Optional[str],
        priority: int
    ) -> Optional[MemoryProposal]:
        """
        Create proposal using template-based rules.
        LLM only used for optional refinement if enabled.

        Args:
            cluster: CorrectionCluster from Phase 2
            project: Optional project name
            priority: Priority level (1=critical, 2=high, 3=medium)

        Returns:
            MemoryProposal or None if generation fails
        """
        # PRIMARY: Template-based proposal generation
        scope = self._infer_scope(cluster, project)
        category = self._infer_category(cluster)
        content = self._generate_content(cluster)
        rationale = self._generate_rationale(cluster)

        # OPTIONAL: LLM refinement
        if self.use_llm:
            refined = self._try_llm_refinement(content, cluster)
            if refined:
                content = refined

        return MemoryProposal(
            id=f"prop-{uuid.uuid4().hex[:8]}",
            category=category,
            scope=scope,
            content=content,
            rationale=rationale,
            source_variants=cluster.variants,
            confidence=cluster.confidence,
            priority=priority
        )

    def _infer_scope(self, cluster, project: Optional[str]) -> str:
        """
        Infer scope from correction content.

        Hierarchy: project > language > universal

        Args:
            cluster: CorrectionCluster
            project: Optional project name

        Returns:
            Scope string: "universal", "language:X", or "project:X"
        """
        text_lower = cluster.canonical_text.lower()

        # Check for language-specific content
        for lang, keywords in self.LANG_KEYWORDS.items():
            if any(kw in text_lower for kw in keywords):
                return f"language:{lang}"

        # Check for project-specific indicators
        if project:
            if any(ind in text_lower for ind in self.PROJECT_INDICATORS):
                return f"project:{project}"

        # Check for universal patterns (applies everywhere)
        if any(kw in text_lower for kw in self.UNIVERSAL_KEYWORDS):
            return "universal"

        # Default to universal (safest)
        return "universal"

    def _infer_category(self, cluster) -> str:
        """
        Infer category from correction type and content.

        Args:
            cluster: CorrectionCluster

        Returns:
            Category: "preference", "rule", or "correction"
        """
        text_lower = cluster.canonical_text.lower()

        # Correction type mapping
        TYPE_MAP = {
            "behavior": "rule",
            "style": "preference",
            "rule": "rule",
            "preference": "preference"
        }

        base_category = TYPE_MAP.get(cluster.correction_type, "preference")

        # Override based on content keywords
        if any(kw in text_lower for kw in ["never", "don't", "avoid", "stop"]):
            return "correction"  # Anti-pattern
        if any(kw in text_lower for kw in ["always", "must", "limit", "max"]):
            return "rule"  # Hard rule
        if any(kw in text_lower for kw in ["prefer", "like", "use", "keep"]):
            return "preference"  # Soft preference

        return base_category

    def _generate_content(self, cluster) -> str:
        """
        Generate content from canonical text using templates.

        Transforms user correction into imperative form.

        Args:
            cluster: CorrectionCluster

        Returns:
            Content string in imperative form
        """
        canonical = cluster.canonical_text.strip()
        words = canonical.lower().split()

        # Already in good form?
        GOOD_STARTERS = {
            'use', 'keep', 'avoid', 'never', 'always', 'prefer',
            'write', 'split', 'plan', 'test', 'log', 'limit',
            'make', 'ensure', 'check', 'validate', 'handle'
        }
        if words and words[0] in GOOD_STARTERS:
            # Capitalize first letter if needed
            return canonical[0].upper() + canonical[1:] if canonical else canonical

        # Transform common patterns
        TRANSFORMATIONS = [
            # "don't X" → "Avoid X"
            (r"^don'?t\s+", "Avoid "),
            # "stop X" → "Avoid X"
            (r"^stop\s+", "Avoid "),
            # "be X" → "Keep output X"
            (r"^be\s+(concise|terse|short|brief)", r"Keep output \1"),
            # "X is bad" → "Avoid X"
            (r"(.+)\s+is\s+bad", r"Avoid \1"),
            # "X is good" → "Prefer X"
            (r"(.+)\s+is\s+good", r"Prefer \1"),
            # "I want X" → "X"
            (r"^i\s+want\s+", ""),
            # "you should X" → "X"
            (r"^you\s+should\s+", ""),
            # "make sure to X" → "X" (if X starts with good verb)
            (r"^make\s+sure\s+to\s+", ""),
        ]

        result = canonical
        for pattern, replacement in TRANSFORMATIONS:
            result = re.sub(pattern, replacement, result, flags=re.IGNORECASE)

        # Capitalize first letter
        if result:
            result = result[0].upper() + result[1:]

        # Ensure reasonable length (max 20 words)
        words_list = result.split()
        if len(words_list) > 20:
            result = ' '.join(words_list[:20]) + '...'

        return result

    def _generate_rationale(self, cluster) -> str:
        """
        Generate rationale from cluster data.

        Args:
            cluster: CorrectionCluster

        Returns:
            Rationale string explaining why this should be stored
        """
        freq = cluster.frequency
        conf = cluster.confidence

        if freq >= 3:
            return f"User corrected this {freq} times (high priority)"
        elif freq == 2:
            return "User corrected this twice"
        elif conf >= 0.9:
            return "Explicit correction with high confidence"
        else:
            return "User indicated preference"

    def _try_llm_refinement(self, content: str, cluster) -> Optional[str]:
        """
        OPTIONAL: Use Ollama for content refinement if available.

        Args:
            content: Generated content string
            cluster: CorrectionCluster

        Returns:
            Refined content or None if LLM unavailable/fails
        """
        try:
            import requests

            prompt = f"""Refine this rule to terse imperative form (max 10 words):

Rule: {content}
Original: {cluster.variants[0] if cluster.variants else content}

Output ONLY the refined rule:"""

            response = requests.post(
                "http://localhost:11434/api/generate",
                json={"model": "llama3.2:3b", "prompt": prompt, "stream": False},
                timeout=5
            )

            if response.status_code == 200:
                refined = response.json().get("response", "").strip()
                # Validate: must be reasonable length
                if 3 <= len(refined.split()) <= 15:
                    return refined

        except Exception:
            pass  # LLM unavailable or failed - that's fine

        return None

    def _gen_session_id(self) -> str:
        """Generate session ID from timestamp."""
        return datetime.now().strftime("%Y%m%d-%H%M%S")


# Module-level convenience functions

def generate_proposals(
    clusters: List,
    project: Optional[str] = None,
    use_llm: bool = False,
    max_proposals: int = 5
) -> SessionSummary:
    """
    Convenience function to generate proposals.

    Args:
        clusters: List of CorrectionCluster from Phase 2
        project: Optional project name for scope inference
        use_llm: Enable optional LLM refinement
        max_proposals: Maximum proposals to generate

    Returns:
        SessionSummary with proposals
    """
    engine = ProposalEngine(use_llm=use_llm, max_proposals=max_proposals)
    return engine.generate_proposals(clusters, project)


def create_test_scenarios():
    """
    Create test scenarios for validation.

    Returns:
        List of test scenario dictionaries
    """
    return [
        {
            "name": "Universal preference",
            "canonical": "Keep summaries concise",
            "correction_type": "style",
            "frequency": 3,
            "confidence": 0.9,
            "expected_scope": "universal",
            "expected_category": "preference"
        },
        {
            "name": "Language-specific rule",
            "canonical": "Never use panic in production Go code",
            "correction_type": "rule",
            "frequency": 2,
            "confidence": 0.95,
            "expected_scope": "language:go",
            "expected_category": "rule"
        },
        {
            "name": "Project-specific rule",
            "canonical": "Use PortalTabs for all portal pages",
            "correction_type": "rule",
            "frequency": 2,
            "confidence": 0.8,
            "project": "xcalibr",
            "expected_scope": "project:xcalibr",
            "expected_category": "rule"
        },
        {
            "name": "Correction (anti-pattern)",
            "canonical": "Don't write verbose explanations",
            "correction_type": "behavior",
            "frequency": 2,
            "confidence": 0.85,
            "expected_scope": "universal",
            "expected_category": "correction"
        },
        {
            "name": "Line limit rule",
            "canonical": "Never exceed 200 lines per file",
            "correction_type": "rule",
            "frequency": 3,
            "confidence": 0.95,
            "expected_scope": "universal",
            "expected_category": "rule"
        }
    ]
